# Test Command

## Trigger Patterns
- `@agent test`
- `@agent write tests`
- `@agent add test coverage`
- PR with test coverage < 80%
- New feature without tests

## Behavior
Generate comprehensive test coverage for code changes or specific modules following TDD principles.

## Parameters
- `--file <path>`: Generate tests for specific file
- `--function <name>`: Test specific function/method
- `--coverage-target <percent>`: Target coverage (default: 80)
- `--type <test_type>`: Test type to generate
  - `unit`: Unit tests only
  - `integration`: Integration tests only
  - `all`: Both unit and integration (default)

## Execution Flow

### 1. Analysis Phase
- Parse target code to identify testable units
- Extract function signatures and dependencies
- Identify edge cases and error conditions
- Check existing test coverage
- Query knowledge graph for dependencies

### 2. Test Generation
- Generate test file structure
- Create test fixtures and mocks
- Write unit tests for each function
- Write integration tests for workflows
- Add edge case and error tests
- Follow existing test patterns

### 3. Verification
- Run generated tests
- Measure coverage
- Verify all tests pass
- Check test quality metrics

### 4. Result Posting
- Post test summary to PR/issue
- Include coverage delta
- List all generated tests

## Test Generation Rules

### Naming Convention
```python
def test_[function]_[scenario]_[expected_result]():
```

Examples:
- `test_process_task_with_valid_input_succeeds`
- `test_post_result_with_invalid_provider_returns_false`
- `test_log_progress_with_closed_logger_does_nothing`

### Test Structure
```python
def test_function_name_scenario():
    fixture = create_test_fixture()

    result = function_under_test(fixture.input)

    assert result == expected_output
    assert fixture.side_effect_occurred()
```

### Edge Cases to Cover
1. Empty inputs ([], {}, "", None)
2. Boundary values (0, -1, max_int)
3. Invalid types
4. Concurrent access (if applicable)
5. Resource exhaustion
6. Network failures (for I/O code)

### Error Conditions
1. Invalid arguments
2. Missing required fields
3. Type mismatches
4. External service failures
5. Timeout scenarios

## Output Format
```markdown
## Test Coverage Report ðŸ§ª

**Generated by:** Test Writer Agent
**Target:** `core/result_poster.py`

### Coverage Summary
- **Before:** 45% (12/27 lines)
- **After:** 92% (25/27 lines)
- **Delta:** +47% âœ…

### Tests Generated: 12

#### Unit Tests (8 tests)
âœ… `test_post_github_pr_result_with_valid_data_posts_comment`
âœ… `test_post_github_pr_result_with_invalid_repo_returns_false`
âœ… `test_post_github_issue_result_posts_to_issue`
âœ… `test_post_jira_result_with_missing_key_returns_false`
âœ… `test_post_slack_result_with_valid_channel_succeeds`
âœ… `test_post_sentry_result_adds_comment`
âœ… `test_post_result_with_unknown_provider_logs_warning`
âœ… `test_post_result_with_exception_returns_false`

#### Integration Tests (2 tests)
âœ… `test_result_poster_with_mcp_client_integration_works`
âœ… `test_multiple_provider_posting_in_sequence`

#### Edge Case Tests (2 tests)
âœ… `test_post_github_result_with_empty_repository_fails`
âœ… `test_post_result_with_none_metadata_handles_gracefully`

### Files Created
- `tests/test_result_poster.py` (+120 lines)

### Uncovered Lines
- Line 89-91: Error logging block (hard to test without real MCP failure)

### Test Execution Results
```
========================= test session starts ==========================
tests/test_result_poster.py::test_post_github_pr_result_with_valid_data_posts_comment PASSED
tests/test_result_poster.py::test_post_github_pr_result_with_invalid_repo_returns_false PASSED
... (10 more passing)
========================= 12 passed in 0.15s ===========================
```

### Quality Metrics
- âœ… All tests pass
- âœ… No test interdependencies
- âœ… Proper fixture usage
- âœ… Meaningful assertions
- âœ… Fast execution (< 1s total)

### Recommendations
1. Consider adding performance tests for large payload scenarios
2. Add property-based tests using Hypothesis for robust validation

---
*Generated tests follow TDD best practices and project conventions*
```

## Test Quality Checklist
- [ ] Tests are independent (no shared state)
- [ ] Tests are deterministic (same input = same output)
- [ ] Tests have clear Given-When-Then structure
- [ ] Fixtures used for common setup
- [ ] Mocks are minimal and focused
- [ ] Async tests use proper pytest-asyncio markers
- [ ] Test names describe scenario and expectation
- [ ] Edge cases covered
- [ ] Error conditions tested
- [ ] Fast execution (unit tests < 100ms each)

## Example Usage

### Generate Tests for File
```
@agent test --file core/result_poster.py
```

### Generate Only Unit Tests
```
@agent test --file core/streaming_logger.py --type unit
```

### Achieve Specific Coverage
```
@agent test --coverage-target 95
```

## Success Criteria
- Coverage target achieved
- All generated tests pass
- Tests follow project conventions
- Test generation completed within 5 minutes
- No flaky or dependent tests

## Escalation Conditions
- Cannot determine expected behavior â†’ Request specification/examples
- Existing tests fail â†’ Report and halt, fix existing tests first
- Coverage target unreachable â†’ Explain why and suggest alternative
- Test execution fails â†’ Debug and report issue
- Complex business logic unclear â†’ Request domain expert consultation
